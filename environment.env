BOT_TOKEN=""            # Telegram bot token
MODEL_PATH= ""          # Path to the model that will be used by llama.cpp
ALLOWED_USERS=""        # If "", all users will be allowed, if you want to restrict separater by comas the userids
GPU_LAYERS=0            # Specify the amount of layers that will be on the gpu
PROMPT_TEMPLATE_FOLDER= "prompts" # Prompts must be stored according to the structure: {folder}/{language}/{prompt_name.prompt}