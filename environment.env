BOT_TOKEN=None      # Telegram bot token
MODEL_PATH=None     # Path to the model that will be used by llama.cpp
ALLOWED_USERS=None  # If no None, all users will be allowed
GPU_LAYERS=None     # Specify the amount of layers that will be on the gpu
PROMPT_TEMPLATE_FOLDER= "prompts" # Prompts must be stored according to the structure: {folder}/{language}/{prompt_name.prompt}